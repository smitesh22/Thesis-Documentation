{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oIA4yqc3yp4n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# atomspheric features for training model\n",
        "\n",
        "dataframe = pd.read_csv(\"/content/drive/My Drive/Data/Features.csv\")"
      ],
      "metadata": {
        "id": "NNi1r7lZ9oHo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting the dataframe into a dict\n",
        "dataframe = dataframe.set_index(\"filename\")\n",
        "dataframe = dataframe.T.to_dict()"
      ],
      "metadata": {
        "id": "WUsGMtio-RBO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the slope embeddings\n",
        "slope_embeddings = torch.load(\"/content/drive/My Drive/Data/Slope_embeddings.pt\")"
      ],
      "metadata": {
        "id": "ARBfBYnXyvcM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the elevation embeddings\n",
        "elevation_embeddings = torch.load(\"/content/drive/My Drive/Data/Elevation_embeddings.pt\")"
      ],
      "metadata": {
        "id": "dDZGUDqfzLff"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in elevation_embeddings:\n",
        "    matrix_min = elevation_embeddings[key].min()\n",
        "    matrix_max = elevation_embeddings[key].max()\n",
        "    elevation_embeddings[key] = (elevation_embeddings[key] - matrix_min) / (matrix_max - matrix_min)\n",
        "\n",
        "for key in slope_embeddings:\n",
        "    matrix_min = slope_embeddings[key].min()\n",
        "    matrix_max = slope_embeddings[key].max()\n",
        "    slope_embeddings[key] = (slope_embeddings[key] - matrix_min) / (matrix_max - matrix_min)\n"
      ],
      "metadata": {
        "id": "hHz2Vr4xTSpM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concat the slope and elevation embeddings generated using encoder decoder\n",
        "combined_dict = {}\n",
        "for key in slope_embeddings.keys():\n",
        "    combined_dict[key] = torch.cat((slope_embeddings[key], elevation_embeddings[key]), dim=1)"
      ],
      "metadata": {
        "id": "-WuTxAp1zf7K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            nn.Linear(2000, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000,500),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            nn.Linear(508, 250),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(250, 508),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(508, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 2000),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, x, features):\n",
        "        x = self.encoder1(x)\n",
        "        #print(\"Before squeeze: \", x.shape, features.shape)\n",
        "        x = x.squeeze(1)\n",
        "        features = features.squeeze(1)\n",
        "        #print(\"After squeeze: \", x.shape, features.shape)\n",
        "        x = torch.cat((x, features), dim=1)\n",
        "        x = self.encoder2(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "E2LYXssR6WZ_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(Dataset):\n",
        "  def __init__(self, data, feature_map):\n",
        "    self.filename = list(data.keys())\n",
        "    self.data = data\n",
        "    self.feature_map = feature_map\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.filename)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    features = self.feature_map[self.filename[index]]\n",
        "    features = torch.FloatTensor([features[key] for key in features]).unsqueeze(0)\n",
        "    return [self.data[self.filename[index]].detach(), features]\n",
        "\n"
      ],
      "metadata": {
        "id": "piibj7QCLxg8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset(combined_dict, dataframe)\n",
        "batch_size = 4\n",
        "train_size = 35\n",
        "test_size = 7\n",
        "val_size = 8\n",
        "\n",
        "train_dataset, test_dataset, val_dataset = random_split(data, [train_size, test_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "16d7LJQMNB8e"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoEncoder()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "n_epochs = 100\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "vskAL1BB_X63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab635e0f-f3cd-4eee-df81-6dc4df94a9cd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoEncoder(\n",
              "  (encoder1): Sequential(\n",
              "    (0): Linear(in_features=2000, out_features=1000, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=1000, out_features=500, bias=True)\n",
              "    (3): ReLU()\n",
              "  )\n",
              "  (encoder2): Sequential(\n",
              "    (0): Linear(in_features=508, out_features=250, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): Linear(in_features=250, out_features=508, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=508, out_features=1000, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=1000, out_features=2000, bias=True)\n",
              "    (5): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 1000\n",
        "patience = 10\n",
        "\n",
        "# track the validation loss from the previous epoch\n",
        "best_valid_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for batch in train_loader:\n",
        "        inputs, features = batch\n",
        "\n",
        "        # Move inputs to the device if using GPU\n",
        "        inputs = inputs.squeeze(1).to(device)\n",
        "        features = features.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs, features)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, inputs)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Compute the validation loss\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for val_batch_data in val_loader:\n",
        "            inputs, features = val_batch_data\n",
        "            #val_batch_data = val_batch_data.to(device)\n",
        "            val_inputs = inputs.squeeze(1).to(device)\n",
        "            val_features = features.to(device)\n",
        "            val_outputs = model(val_inputs, val_features)\n",
        "            val_loss += criterion(val_outputs, val_inputs).item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    if val_loss < best_valid_loss:\n",
        "        best_valid_loss = val_loss\n",
        "        counter = 0  # reset counter\n",
        "    else:\n",
        "        counter += 1  # increment counter if validation loss has not improved\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}, Validation Loss: {:.4f}'\n",
        "          .format(epoch+1, n_epochs, loss.item(), val_loss))\n",
        "\n",
        "    # If the validation loss hasn't improved in `patience` epochs, stop training early\n",
        "    if counter == patience:\n",
        "        print(\"Early stopping\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "ZOJC1dhDAuoh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "175ec892-b088-4f5d-8077-bcc0938af2a4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000], Loss: 0.2233, Validation Loss: 0.2087\n",
            "Epoch [2/1000], Loss: 0.1949, Validation Loss: 0.1923\n",
            "Epoch [3/1000], Loss: 0.1855, Validation Loss: 0.1772\n",
            "Epoch [4/1000], Loss: 0.1700, Validation Loss: 0.1630\n",
            "Epoch [5/1000], Loss: 0.1490, Validation Loss: 0.1493\n",
            "Epoch [6/1000], Loss: 0.1362, Validation Loss: 0.1358\n",
            "Epoch [7/1000], Loss: 0.1343, Validation Loss: 0.1224\n",
            "Epoch [8/1000], Loss: 0.1094, Validation Loss: 0.1091\n",
            "Epoch [9/1000], Loss: 0.1034, Validation Loss: 0.0959\n",
            "Epoch [10/1000], Loss: 0.0890, Validation Loss: 0.0831\n",
            "Epoch [11/1000], Loss: 0.0668, Validation Loss: 0.0708\n",
            "Epoch [12/1000], Loss: 0.0707, Validation Loss: 0.0596\n",
            "Epoch [13/1000], Loss: 0.0540, Validation Loss: 0.0495\n",
            "Epoch [14/1000], Loss: 0.0463, Validation Loss: 0.0408\n",
            "Epoch [15/1000], Loss: 0.0353, Validation Loss: 0.0335\n",
            "Epoch [16/1000], Loss: 0.0313, Validation Loss: 0.0276\n",
            "Epoch [17/1000], Loss: 0.0291, Validation Loss: 0.0231\n",
            "Epoch [18/1000], Loss: 0.0188, Validation Loss: 0.0195\n",
            "Epoch [19/1000], Loss: 0.0213, Validation Loss: 0.0168\n",
            "Epoch [20/1000], Loss: 0.0148, Validation Loss: 0.0148\n",
            "Epoch [21/1000], Loss: 0.0208, Validation Loss: 0.0133\n",
            "Epoch [22/1000], Loss: 0.0187, Validation Loss: 0.0123\n",
            "Epoch [23/1000], Loss: 0.0146, Validation Loss: 0.0116\n",
            "Epoch [24/1000], Loss: 0.0138, Validation Loss: 0.0110\n",
            "Epoch [25/1000], Loss: 0.0092, Validation Loss: 0.0105\n",
            "Epoch [26/1000], Loss: 0.0091, Validation Loss: 0.0101\n",
            "Epoch [27/1000], Loss: 0.0133, Validation Loss: 0.0099\n",
            "Epoch [28/1000], Loss: 0.0119, Validation Loss: 0.0097\n",
            "Epoch [29/1000], Loss: 0.0157, Validation Loss: 0.0095\n",
            "Epoch [30/1000], Loss: 0.0102, Validation Loss: 0.0092\n",
            "Epoch [31/1000], Loss: 0.0128, Validation Loss: 0.0093\n",
            "Epoch [32/1000], Loss: 0.0129, Validation Loss: 0.0091\n",
            "Epoch [33/1000], Loss: 0.0133, Validation Loss: 0.0090\n",
            "Epoch [34/1000], Loss: 0.0059, Validation Loss: 0.0090\n",
            "Epoch [35/1000], Loss: 0.0085, Validation Loss: 0.0090\n",
            "Epoch [36/1000], Loss: 0.0133, Validation Loss: 0.0087\n",
            "Epoch [37/1000], Loss: 0.0054, Validation Loss: 0.0087\n",
            "Epoch [38/1000], Loss: 0.0106, Validation Loss: 0.0086\n",
            "Epoch [39/1000], Loss: 0.0088, Validation Loss: 0.0086\n",
            "Epoch [40/1000], Loss: 0.0098, Validation Loss: 0.0085\n",
            "Epoch [41/1000], Loss: 0.0093, Validation Loss: 0.0085\n",
            "Epoch [42/1000], Loss: 0.0181, Validation Loss: 0.0085\n",
            "Epoch [43/1000], Loss: 0.0119, Validation Loss: 0.0085\n",
            "Epoch [44/1000], Loss: 0.0050, Validation Loss: 0.0086\n",
            "Epoch [45/1000], Loss: 0.0051, Validation Loss: 0.0085\n",
            "Epoch [46/1000], Loss: 0.0064, Validation Loss: 0.0085\n",
            "Epoch [47/1000], Loss: 0.0144, Validation Loss: 0.0084\n",
            "Epoch [48/1000], Loss: 0.0109, Validation Loss: 0.0084\n",
            "Epoch [49/1000], Loss: 0.0068, Validation Loss: 0.0085\n",
            "Epoch [50/1000], Loss: 0.0059, Validation Loss: 0.0085\n",
            "Epoch [51/1000], Loss: 0.0069, Validation Loss: 0.0084\n",
            "Epoch [52/1000], Loss: 0.0043, Validation Loss: 0.0083\n",
            "Epoch [53/1000], Loss: 0.0125, Validation Loss: 0.0082\n",
            "Epoch [54/1000], Loss: 0.0097, Validation Loss: 0.0083\n",
            "Epoch [55/1000], Loss: 0.0091, Validation Loss: 0.0082\n",
            "Epoch [56/1000], Loss: 0.0062, Validation Loss: 0.0083\n",
            "Epoch [57/1000], Loss: 0.0059, Validation Loss: 0.0083\n",
            "Epoch [58/1000], Loss: 0.0169, Validation Loss: 0.0082\n",
            "Epoch [59/1000], Loss: 0.0067, Validation Loss: 0.0082\n",
            "Epoch [60/1000], Loss: 0.0112, Validation Loss: 0.0082\n",
            "Epoch [61/1000], Loss: 0.0116, Validation Loss: 0.0082\n",
            "Epoch [62/1000], Loss: 0.0077, Validation Loss: 0.0083\n",
            "Epoch [63/1000], Loss: 0.0099, Validation Loss: 0.0082\n",
            "Epoch [64/1000], Loss: 0.0108, Validation Loss: 0.0081\n",
            "Epoch [65/1000], Loss: 0.0102, Validation Loss: 0.0081\n",
            "Epoch [66/1000], Loss: 0.0055, Validation Loss: 0.0081\n",
            "Epoch [67/1000], Loss: 0.0113, Validation Loss: 0.0082\n",
            "Epoch [68/1000], Loss: 0.0148, Validation Loss: 0.0082\n",
            "Epoch [69/1000], Loss: 0.0047, Validation Loss: 0.0082\n",
            "Epoch [70/1000], Loss: 0.0062, Validation Loss: 0.0080\n",
            "Epoch [71/1000], Loss: 0.0080, Validation Loss: 0.0081\n",
            "Epoch [72/1000], Loss: 0.0169, Validation Loss: 0.0081\n",
            "Epoch [73/1000], Loss: 0.0120, Validation Loss: 0.0080\n",
            "Epoch [74/1000], Loss: 0.0135, Validation Loss: 0.0081\n",
            "Epoch [75/1000], Loss: 0.0068, Validation Loss: 0.0081\n",
            "Epoch [76/1000], Loss: 0.0126, Validation Loss: 0.0080\n",
            "Epoch [77/1000], Loss: 0.0107, Validation Loss: 0.0080\n",
            "Epoch [78/1000], Loss: 0.0119, Validation Loss: 0.0080\n",
            "Epoch [79/1000], Loss: 0.0078, Validation Loss: 0.0079\n",
            "Epoch [80/1000], Loss: 0.0055, Validation Loss: 0.0080\n",
            "Epoch [81/1000], Loss: 0.0118, Validation Loss: 0.0081\n",
            "Epoch [82/1000], Loss: 0.0042, Validation Loss: 0.0081\n",
            "Epoch [83/1000], Loss: 0.0045, Validation Loss: 0.0079\n",
            "Epoch [84/1000], Loss: 0.0107, Validation Loss: 0.0079\n",
            "Epoch [85/1000], Loss: 0.0080, Validation Loss: 0.0079\n",
            "Epoch [86/1000], Loss: 0.0066, Validation Loss: 0.0078\n",
            "Epoch [87/1000], Loss: 0.0110, Validation Loss: 0.0078\n",
            "Epoch [88/1000], Loss: 0.0096, Validation Loss: 0.0078\n",
            "Epoch [89/1000], Loss: 0.0049, Validation Loss: 0.0078\n",
            "Epoch [90/1000], Loss: 0.0097, Validation Loss: 0.0078\n",
            "Epoch [91/1000], Loss: 0.0051, Validation Loss: 0.0077\n",
            "Epoch [92/1000], Loss: 0.0087, Validation Loss: 0.0077\n",
            "Epoch [93/1000], Loss: 0.0121, Validation Loss: 0.0078\n",
            "Epoch [94/1000], Loss: 0.0129, Validation Loss: 0.0078\n",
            "Epoch [95/1000], Loss: 0.0088, Validation Loss: 0.0077\n",
            "Epoch [96/1000], Loss: 0.0042, Validation Loss: 0.0077\n",
            "Epoch [97/1000], Loss: 0.0061, Validation Loss: 0.0076\n",
            "Epoch [98/1000], Loss: 0.0066, Validation Loss: 0.0076\n",
            "Epoch [99/1000], Loss: 0.0038, Validation Loss: 0.0076\n",
            "Epoch [100/1000], Loss: 0.0051, Validation Loss: 0.0075\n",
            "Epoch [101/1000], Loss: 0.0059, Validation Loss: 0.0075\n",
            "Epoch [102/1000], Loss: 0.0058, Validation Loss: 0.0075\n",
            "Epoch [103/1000], Loss: 0.0092, Validation Loss: 0.0075\n",
            "Epoch [104/1000], Loss: 0.0078, Validation Loss: 0.0075\n",
            "Epoch [105/1000], Loss: 0.0094, Validation Loss: 0.0075\n",
            "Epoch [106/1000], Loss: 0.0099, Validation Loss: 0.0074\n",
            "Epoch [107/1000], Loss: 0.0073, Validation Loss: 0.0074\n",
            "Epoch [108/1000], Loss: 0.0069, Validation Loss: 0.0074\n",
            "Epoch [109/1000], Loss: 0.0064, Validation Loss: 0.0074\n",
            "Epoch [110/1000], Loss: 0.0050, Validation Loss: 0.0073\n",
            "Epoch [111/1000], Loss: 0.0060, Validation Loss: 0.0072\n",
            "Epoch [112/1000], Loss: 0.0049, Validation Loss: 0.0073\n",
            "Epoch [113/1000], Loss: 0.0092, Validation Loss: 0.0073\n",
            "Epoch [114/1000], Loss: 0.0138, Validation Loss: 0.0073\n",
            "Epoch [115/1000], Loss: 0.0078, Validation Loss: 0.0073\n",
            "Epoch [116/1000], Loss: 0.0077, Validation Loss: 0.0072\n",
            "Epoch [117/1000], Loss: 0.0052, Validation Loss: 0.0072\n",
            "Epoch [118/1000], Loss: 0.0040, Validation Loss: 0.0071\n",
            "Epoch [119/1000], Loss: 0.0062, Validation Loss: 0.0072\n",
            "Epoch [120/1000], Loss: 0.0068, Validation Loss: 0.0072\n",
            "Epoch [121/1000], Loss: 0.0070, Validation Loss: 0.0071\n",
            "Epoch [122/1000], Loss: 0.0049, Validation Loss: 0.0071\n",
            "Epoch [123/1000], Loss: 0.0070, Validation Loss: 0.0071\n",
            "Epoch [124/1000], Loss: 0.0081, Validation Loss: 0.0071\n",
            "Epoch [125/1000], Loss: 0.0073, Validation Loss: 0.0071\n",
            "Epoch [126/1000], Loss: 0.0065, Validation Loss: 0.0070\n",
            "Epoch [127/1000], Loss: 0.0044, Validation Loss: 0.0070\n",
            "Epoch [128/1000], Loss: 0.0052, Validation Loss: 0.0071\n",
            "Epoch [129/1000], Loss: 0.0069, Validation Loss: 0.0070\n",
            "Epoch [130/1000], Loss: 0.0036, Validation Loss: 0.0070\n",
            "Epoch [131/1000], Loss: 0.0025, Validation Loss: 0.0069\n",
            "Epoch [132/1000], Loss: 0.0045, Validation Loss: 0.0070\n",
            "Epoch [133/1000], Loss: 0.0110, Validation Loss: 0.0070\n",
            "Epoch [134/1000], Loss: 0.0059, Validation Loss: 0.0070\n",
            "Epoch [135/1000], Loss: 0.0045, Validation Loss: 0.0070\n",
            "Epoch [136/1000], Loss: 0.0058, Validation Loss: 0.0069\n",
            "Epoch [137/1000], Loss: 0.0096, Validation Loss: 0.0069\n",
            "Epoch [138/1000], Loss: 0.0027, Validation Loss: 0.0070\n",
            "Epoch [139/1000], Loss: 0.0038, Validation Loss: 0.0069\n",
            "Epoch [140/1000], Loss: 0.0072, Validation Loss: 0.0069\n",
            "Epoch [141/1000], Loss: 0.0045, Validation Loss: 0.0069\n",
            "Epoch [142/1000], Loss: 0.0035, Validation Loss: 0.0069\n",
            "Epoch [143/1000], Loss: 0.0046, Validation Loss: 0.0069\n",
            "Epoch [144/1000], Loss: 0.0045, Validation Loss: 0.0069\n",
            "Epoch [145/1000], Loss: 0.0056, Validation Loss: 0.0070\n",
            "Epoch [146/1000], Loss: 0.0054, Validation Loss: 0.0069\n",
            "Epoch [147/1000], Loss: 0.0099, Validation Loss: 0.0069\n",
            "Epoch [148/1000], Loss: 0.0051, Validation Loss: 0.0068\n",
            "Epoch [149/1000], Loss: 0.0039, Validation Loss: 0.0068\n",
            "Epoch [150/1000], Loss: 0.0062, Validation Loss: 0.0069\n",
            "Epoch [151/1000], Loss: 0.0055, Validation Loss: 0.0068\n",
            "Epoch [152/1000], Loss: 0.0049, Validation Loss: 0.0068\n",
            "Epoch [153/1000], Loss: 0.0073, Validation Loss: 0.0069\n",
            "Epoch [154/1000], Loss: 0.0064, Validation Loss: 0.0069\n",
            "Epoch [155/1000], Loss: 0.0032, Validation Loss: 0.0069\n",
            "Epoch [156/1000], Loss: 0.0038, Validation Loss: 0.0068\n",
            "Epoch [157/1000], Loss: 0.0051, Validation Loss: 0.0068\n",
            "Epoch [158/1000], Loss: 0.0043, Validation Loss: 0.0068\n",
            "Epoch [159/1000], Loss: 0.0052, Validation Loss: 0.0067\n",
            "Epoch [160/1000], Loss: 0.0127, Validation Loss: 0.0069\n",
            "Epoch [161/1000], Loss: 0.0033, Validation Loss: 0.0067\n",
            "Epoch [162/1000], Loss: 0.0038, Validation Loss: 0.0067\n",
            "Epoch [163/1000], Loss: 0.0062, Validation Loss: 0.0068\n",
            "Epoch [164/1000], Loss: 0.0036, Validation Loss: 0.0068\n",
            "Epoch [165/1000], Loss: 0.0084, Validation Loss: 0.0069\n",
            "Epoch [166/1000], Loss: 0.0023, Validation Loss: 0.0067\n",
            "Epoch [167/1000], Loss: 0.0052, Validation Loss: 0.0067\n",
            "Epoch [168/1000], Loss: 0.0036, Validation Loss: 0.0068\n",
            "Epoch [169/1000], Loss: 0.0107, Validation Loss: 0.0068\n",
            "Epoch [170/1000], Loss: 0.0029, Validation Loss: 0.0067\n",
            "Epoch [171/1000], Loss: 0.0086, Validation Loss: 0.0067\n",
            "Epoch [172/1000], Loss: 0.0083, Validation Loss: 0.0067\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # set the model to evaluation mode\n",
        "test_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for test_batch_data in test_loader:\n",
        "        test_inputs, features = test_batch_data\n",
        "        test_inputs = test_inputs.squeeze(1).to(device)\n",
        "        features = features.squeeze(1).to(device)\n",
        "        test_outputs = model(test_inputs, features)\n",
        "        test_loss += criterion(test_outputs, test_inputs).item()\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "print('Test Loss: {:.4f}'.format(test_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUZMDk6sOhN7",
        "outputId": "9c26f864-9758-4a35-d436-6dc3ea5bb70a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"/content/drive/My Drive/Thesis/Model/LinearAutoEncoder.pt\")"
      ],
      "metadata": {
        "id": "wRccdZVHv5sR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eOf13bpRyZc-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}